{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1: DATA LOADING AND PREPROCESSING\n",
        "**bold text**"
      ],
      "metadata": {
        "id": "Vo7UnbyStqWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kfcV-WgdtV3B"
      },
      "outputs": [],
      "source": [
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filters out invalid json objects and saves the remaining in output file. the try block might\n",
        "#raise an exception so to handle that the except block executes\n",
        "\n",
        "json_file = '/content/Cell_Phones_and_Accessories_5.json'\n",
        "output_json_list = []\n",
        "\n",
        "try:\n",
        "    with open(json_file, 'r') as file:\n",
        "        for line in file:\n",
        "            try:\n",
        "                python_data = json.loads(line)\n",
        "                output_json_list.append(python_data)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Ignoring line}\")\n",
        "\n",
        "    for data in output_json_list:\n",
        "        print(data)\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found.\")\n",
        "except Exception as e:\n",
        "    print(\"An unexpected error occurred\")\n"
      ],
      "metadata": {
        "id": "ekjZPv3G2Bwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#structure of the dataset where the key from the key-value pair is printed\n",
        "if output_json_list:\n",
        "        first_record = output_json_list[0]\n",
        "        print(\"Keys in the first record:\", list(first_record.keys()))\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "#total number of records in the dataset by printing length of it\n",
        "num_records = len(output_json_list)\n",
        "print(\"Number of records:\", num_records)\n",
        "\n",
        "#printing the data types where items function is used to iterate over key-value pair\n",
        "if output_json_list:\n",
        "        first_record = output_json_list[0]\n",
        "        second_record = output_json_list[1]\n",
        "        third_record = output_json_list[2]\n",
        "        fourth_record = output_json_list[3]\n",
        "        fifth_record = output_json_list[4]\n",
        "\n",
        "\n",
        "        print(\"\\nStructure of the first record:\")\n",
        "        for key, value in first_record.items():\n",
        "            print(key, type(value))\n",
        "\n",
        "        print(\"\\nStructure of the second record:\")\n",
        "        for key, value in second_record.items():\n",
        "            print(key, type(value))\n",
        "\n",
        "        print(\"\\nStructure of the thrid record:\")\n",
        "        for key, value in third_record.items():\n",
        "            print(key, type(value))\n",
        "\n",
        "        print(\"\\nStructure of the fourth record:\")\n",
        "        for key, value in fourth_record.items():\n",
        "            print(key, type(value))\n",
        "\n",
        "        print(\"\\nStructure of the fifth record:\")\n",
        "        for key, value in fifth_record.items():\n",
        "            print(key, type(value))\n"
      ],
      "metadata": {
        "id": "zuqQbIpw3cAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering out columns to keep necessary information to avoid unnecessary computation error\n",
        "columns = ['reviewText', 'overall', 'summary']\n",
        "filtered_dataset=[]\n",
        "for record in output_json_list:\n",
        "    filtered_data = {}\n",
        "    for key in columns:\n",
        "        filtered_data[key] = record[key]\n",
        "    filtered_dataset.append(filtered_data)\n",
        "\n",
        "print(\"The Filtered Dataset is as shown:\")\n",
        "for i in range(min(50, len(filtered_dataset))):\n",
        "\n",
        "    print(filtered_dataset[i])\n"
      ],
      "metadata": {
        "id": "DXpMF13j9otT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying text preprocessing techniques to remove punctuations, stop word and lower the case\n",
        "def is_punctuation(char):\n",
        "    return char in {'!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~'}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    filtered_text = ''\n",
        "    for char in text:\n",
        "        if not is_punctuation(char):\n",
        "            filtered_text += char\n",
        "    text = filtered_text\n",
        "\n",
        "    stopwords_list = [\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n",
        "    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n",
        "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',\n",
        "    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n",
        "    'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
        "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
        "    'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',\n",
        "    'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',\n",
        "    'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
        "\n",
        "    words = text.split()\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "     if word not in stopwords_list:\n",
        "        filtered_words.append(word)\n",
        "\n",
        "    words = filtered_words\n",
        "    text = ''\n",
        "    for word in words:\n",
        "       text += word + ' '\n",
        "    print(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "for record in output_json_list:\n",
        "    filtered_data = {}\n",
        "    for key in columns:\n",
        "        if key == 'reviewText':\n",
        "          record[key] = preprocess_text(record[key])\n",
        "        filtered_data[key] = record[key]\n",
        "\n",
        "        if key == 'summary':\n",
        "          record[key] = preprocess_text(record[key])\n",
        "        filtered_data[key] = record[key]\n",
        "\n",
        "    filtered_dataset.append(filtered_data)\n",
        "\n",
        "print(\"Filtered Dataset:\")\n",
        "for i in range(min(50, len(filtered_dataset))):\n",
        "    print(filtered_dataset[i])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qpbb8OPUCjcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: THEMATIC ANALYSIS\n"
      ],
      "metadata": {
        "id": "ohLmpOOelvew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_sentiment(text):\n",
        "    positive_sentiment = ['good', 'great', 'excellent', 'positive', 'happy','nice','amazing']\n",
        "    negative_sentiment = ['bad', 'poor', 'negative', 'disappointing', 'unhappy','low','fake']\n",
        "\n",
        "    positive_count=0\n",
        "    for word in preprocess_text(text).split():\n",
        "     if word in positive_sentiment:\n",
        "      positive_count += 1\n",
        "     else:\n",
        "      positive_count=0\n",
        "\n",
        "    negative_count=0\n",
        "    for word in preprocess_text(text).split():\n",
        "     if word in negative_sentiment:\n",
        "      negative_count +=1\n",
        "     else:\n",
        "      negative_count=0\n",
        "\n",
        "    if positive_count > negative_count:\n",
        "     return 'positive'\n",
        "    else:\n",
        "     return 'negative'\n",
        "\n",
        "\n",
        "def identify_keyphrases(text, num_phrases=5):\n",
        "    words = text.split()\n",
        "    key_phrases = {}\n",
        "    for word in words:\n",
        "        key_phrases[word] = key_phrases.get(word, 0) + 1\n",
        "    sorted_key_phrases = sorted(key_phrases.items(), key=lambda x: x[1], reverse=True)[:num_phrases]\n",
        "    return sorted_key_phrases\n",
        "\n",
        "columns = ['reviewText', 'summary', 'overall']\n",
        "filtered_dataset = []\n",
        "for record in output_json_list:\n",
        "    filtered_data = {}\n",
        "    for key in columns:\n",
        "        #record[key] = preprocess_text(record[key])\n",
        "        if key == 'reviewText':\n",
        "            key_phrases = identify_keyphrases(record[key])\n",
        "            filtered_data['key_phrases'] = key_phrases\n",
        "        elif key == 'summary':\n",
        "            key_phrases_summary = identify_keyphrases(record[key])\n",
        "            filtered_data['key_phrases_summary'] = key_phrases_summary\n",
        "        filtered_data[key] = record[key]\n",
        "\n",
        "    sentiment = identify_sentiment(record['reviewText'])\n",
        "    filtered_data['sentiment'] = sentiment\n",
        "\n",
        "    filtered_dataset.append(filtered_data)\n",
        "\n",
        "\n",
        "print(\"Filtered Dataset:\")\n",
        "for i in range(min(50, len(filtered_dataset))):\n",
        "    print(filtered_dataset[i])"
      ],
      "metadata": {
        "id": "dd6MWwkRltZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_sentiment(overall_rating):\n",
        "#assuming ratings greater than 5 are positive, else negative\n",
        "    return 'positive' if overall_rating > 5 else 'negative'\n",
        "\n",
        "def identify_keyphrases(text, positive_phrases, negative_phrases, sentiment):\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if sentiment == 'positive':\n",
        "            positive_phrases[word] = positive_phrases.get(word, 0) + 1\n",
        "        else:\n",
        "            negative_phrases[word] = negative_phrases.get(word, 0) + 1\n",
        "\n",
        "columns = ['reviewText', 'summary', 'overall']\n",
        "positive_keywords = {}\n",
        "negative_keywords = {}\n",
        "\n",
        "for record in output_json_list:\n",
        "    sentiment = identify_sentiment(record['overall'])\n",
        "    for key in columns:\n",
        "        if key == 'reviewText':\n",
        "            identify_keyphrases(record[key], positive_keywords, negative_keywords, sentiment)\n",
        "        elif key == 'summary':\n",
        "            identify_keyphrases(record[key], positive_keywords, negative_keywords, sentiment)\n",
        "\n",
        "#xxtract a few words from both positive and negative keywords\n",
        "positive_keywords = [word for word, count in positive_keywords.items() if count > 1]\n",
        "negative_keywords = [word for word, count in negative_keywords.items() if count > 1]\n",
        "\n",
        "#calculate the CDF for positive and negative keywords\n",
        "positive_cdf = {word: sum(positive_keywords.count(w) for w in positive_keywords) for word in positive_keywords}\n",
        "negative_cdf = {word: sum(negative_keywords.count(w) for w in negative_keywords) for word in negative_keywords}\n",
        "\n",
        "print(\"Mixed Keywords:\", positive_keywords + negative_keywords)\n",
        "print(\"\\nPositive CDF:\", positive_cdf)\n",
        "print(\"\\nNegative CDF:\", negative_cdf)\n"
      ],
      "metadata": {
        "id": "nDGwT8CHV4FL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "1096369d-c82a-4f32-a949-8a68c65fc649"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-9746e8a12bd0>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#calculate the CDF for positive and negative keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mpositive_cdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_keywords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_keywords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mnegative_cdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_keywords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_keywords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mixed Keywords:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_keywords\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegative_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-9746e8a12bd0>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#calculate the CDF for positive and negative keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mpositive_cdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_keywords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_keywords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mnegative_cdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_keywords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_keywords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mixed Keywords:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_keywords\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegative_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-9746e8a12bd0>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#calculate the CDF for positive and negative keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mpositive_cdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_keywords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_keywords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mnegative_cdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_keywords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_keywords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mixed Keywords:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_keywords\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegative_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEMANTIC ANALYSIS\n"
      ],
      "metadata": {
        "id": "TtwdqMGe-ory"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rule_based_sentiment_analysis(text, positive_weights, negative_weights):\n",
        "    words = text.split()\n",
        "\n",
        "    sentiment_score = 0\n",
        "    for word in words:\n",
        "        sentiment_score += positive_weights.get(word, 0) - negative_weights.get(word, 0)\n",
        "\n",
        "    if sentiment_score > 0:\n",
        "        sentiment = 'positive'\n",
        "    elif sentiment_score < 0:\n",
        "        sentiment = 'negative'\n",
        "    else:\n",
        "        sentiment = 'neutral'\n",
        "\n",
        "    return sentiment, sentiment_score\n",
        "\n",
        "#weights depending upon positive and negative words\n",
        "positive_weights = {'good': 2, 'great': 3, 'excellent': 3, 'positive': 2, 'happy': 2, 'nice': 2, 'amazing': 4}\n",
        "negative_weights = {'bad': -2, 'poor': -3, 'negative': -2, 'disappointing': -3, 'unhappy': -2, 'low': -2, 'fake': -3}\n",
        "\n",
        "columns = ['reviewText', 'summary', 'overall']\n",
        "filtered_dataset = []\n",
        "\n",
        "for record in output_json_list:\n",
        "    filtered_data = {}\n",
        "    for key in columns:\n",
        "        filtered_data[key] = record[key]\n",
        "\n",
        "#Identify sentiment based on 'reviewText' column using rule-based analysis\n",
        "    sentiment, sentiment_score = rule_based_sentiment_analysis(record['reviewText'], positive_weights, negative_weights)\n",
        "    filtered_data['sentiment'] = sentiment\n",
        "    filtered_data['sentiment_score'] = sentiment_score\n",
        "\n",
        "    filtered_dataset.append(filtered_data)\n",
        "\n",
        "for i in range(min(100, len(filtered_dataset))):\n",
        "    print(\"Review Text:\", filtered_dataset[i]['reviewText'])\n",
        "    print(\"Overall:\", filtered_dataset[i]['overall'])\n",
        "    print(\"Summary:\", filtered_dataset[i]['summary'])\n",
        "    print(\"Sentiment:\", filtered_dataset[i]['sentiment'])\n",
        "    print(\"Sentiment Score:\", filtered_dataset[i]['sentiment_score'])\n",
        "    print(\"\\n---\\n\")\n"
      ],
      "metadata": {
        "id": "S-ruHlg8-D2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the text is saved to an other output file\n",
        "output_file_path = 'sentiment_results.txt'\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for data in filtered_dataset:\n",
        "        output_file.write(f\"Review Text: {data['reviewText']}\\n\")\n",
        "        output_file.write(f\"Sentiment: {data['sentiment']}\\n\")\n",
        "        output_file.write(\"\\n---\\n\")\n",
        "\n",
        "print(f\"Results saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzZV6sgC-Yh5",
        "outputId": "dbe288a7-465f-4a85-cf94-745f80331e0e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to sentiment_results.txt\n"
          ]
        }
      ]
    }
  ]
}